* # 部分Ⅰ 人工智能

  * ## 第1章 基于照片的角色捏脸和个性化技术 

  * ## 第2章 强化学习在游戏AI中的应用

    本章介绍了强化学习方法在制作游戏 AI 方面的一些探索。利用强化学习方法，只需要部分的人工参与，就可以在竞速类、格斗类等游戏上建立快速的高质量游戏AI生成流程。这个流程不仅能够生产在竞技水平上与一流的人类玩家相匹配的AI，还在能力分级、拟人行为等方面做了许多适应实际游戏开发需要的工作。

    - ﻿基于强化学习方法解决了业务冷启动问题。
    - ﻿通过对算法和分布式系统的优化，提升了模型的训练效率，一定程度上解决了机器学习模型训练时间成本高昂的问题。
    - ﻿在自对弈策略、奖励设计等方面形成了设计模板，可以快速实现不同风格玩法的 Bot。
    - ﻿通过奖励设计、动作优化、利用示例数据等手段解决强化学习方法的拟人化问题。
    - ﻿通过改变输入状态、动作优化等方式实现模型分级，以适配不同水平玩家的需求。
    - ﻿实现了包括离线训练、模型评估、模型管理、数据分析和发布的一套完整系统，并通过系统 API实现自定义的自动化工作流。

  * ## 第3章 多种机器学习方法在赛车AI中的综合应用

    1. 本章首先介绍了常规赛车AI 的赛道表示和行驶控制方法，引出了该模型下的调参问题，并提出了遗传算法进行调参的解决方案；然后分别介绍了监督学习和强化学习的基础概念和原理，并对这两个方案面临的问题与挑战进行了简单分析。

    2. 监督学习和强化学习都是机器学习的方法，训练出来的神经网络模型对于大部分研发团队来说是一个黑盒，无法对里面进行精细化控制，较难满足各种应用场景。此外，游戏逻辑变更或新增额外功能，都可能导致训练出来的模型失效，需要重新建模训练。因此，建议在游戏项目初期选用传统方法搭建基础的游戏 AI，在核心玩法稳定的情况下再尝试机器学习的方法。但在核心玩法复杂的竞技游戏中，用传统方法开发高水平的AI 是件非常困难的事情，所以机器学习仍是一种非常不错的解决方案。

    3. 强化学习是学术界在游戏AI研究方面热门的方向之一，随着技术的提升和推广，会在越来越多的游戏中落地，提高行业水平。这些超高水平的游戏 AI 研发技术可以给游戏体验和玩法探索带来更多的可能性。

    4. 对于赛车AI 参数优化问题，可以根据遗传算法流程设计如图所示的训练流程。

       （1）随机初始化n套赛车参数作为初始种群。

       （2）每套参数放到同一辆车上在赛道上跑，计算成绩（适应度）。

       （3）如果有足够优秀的成绩，则输出对应的参数作为结果。

       （4）从n套参数中，按成绩的比例权重随机选择2套参数作为母体。

       （5）把选中的2套参数进行交叉和变异得到新的2套参数。

       （6） 把作为母体的2套参数放回原种群，新的2套参数放入新种群。

       （7） 重复步骤（4）～步骤（6）共n/2次，共得到新的n套参数。

       （8）用新的n套参数重复步骤（2）～步骤（7），直到得到符合条件的参数。

       <img src="./1.png" style="zoom:25%;" />

    5. 监督学习方案是让AI 学习人类玩家的操作的，因此AI 的能力上限理论上无法超越样本中的玩家。同时样本的选取会极大地影响AI 的能力表现。如果全都使用高端玩家录像的样本（失误少），则缺乏异常样本，训练出来的AI 应对异常突发情况的能力比较弱。如果加入中低端玩家的样本，则会拉低整体 AI 的能力水平。此外，在大部分赛道中，只在某些弯道上使用高阶技巧，因此高阶技巧的样本在整个赛道中的占比少，在训练时容易被忽略。所以在实际项目中，需要对样本进行分类和筛选，并且根据游戏特性设计复杂的网络结构，而不是简单地使用一个全连接网络。例如，先把操控左右和漂移的按键抽象成一个独立的模型，把复位刹车操作抽象成另外的模型，再把两个模型结合起来，从而控制赛车。

       除此之外，监督学习方案必须要使用样本来训练模型。一些还没上线的游戏，由于缺少游戏样本，因此无法使用这个方案。

    6. 强化学习过程看起来简单，但是实际落地在项目时，会有很多需要处理的问题。例如，一个动作的价值需要一段时间才能体现出来，Reward 只是当前动作结束后的反馈结果，不能代表动作的长远价值，那么动作的长远价值如何体现？这里面就涉及 Bellman 方程和 Reward 回传的问题。因赛车游戏只看完成赛道的时间，不关心中间过程，只有冲线后才能得到 Reward，所以 Reward 回传的路径很长，需要大量的训练时间，才能回传到初始状态。此外，AI 只追求最终结果，不关心具体操作情况，所以最终训练出来的AI 可能操作频率非常高，超出人类操作的极限。还有可能出现一些操作或失误不像人类的情况。这些问题都需要花费时间进行优化，游戏才能达到较好的上线效果。虽然强化学习技术的挑战较大，但是它可以突破传统游戏 AI 方法的限制，探索出超越人类顶尖玩家的 AI，同时发掘更多新奇的玩法体系。通过大量的探索训练，我们可以发现游戏漏洞或平衡性问题，可以辅助验证新赛车、新角色的数值合理性。

  * ## 第4章 数字人级别的语音驱动面部动画生成

* # 部分Ⅱ 计算机图形

  * ## 第5章 实时面光源渲染

  * ## 第6章 可定制的快速自动化全局光照和可见性烘焙器

  * ## 第7章 物质点法在动画特效中的应用

  * ## 第8章 高自由度捏脸的表情动画复用方案

* # 部分Ⅲ 动画和物理

  * ## 第9章 多足机甲运动控制解决方案

  * ## 第10章 物理查询介绍及玩法应用

  * ## 第11章 基于物理的角色翻越攀爬通用解决方

* # 部分Ⅳ 客户端架构和技术

  * ## 第12章 跨游戏引擎的H5渲染解决方案

  * ## 第13章 大世界的场景复杂度管理方案

  * ## 第14章 基于多级细节网格的场景动态加载

* # 部分Ⅴ 服务端架构和技术

  * ## 第15章 面向游戏的高性能服务网格TbusppMesh

  * ## 第16章 游戏配置系统设计

  * ## 第17章 游戏敏捷运营体系技术

* # 部分Ⅵ 管线和工具

  * ## 第18章 从照片到模型

  * ## 第19章 一种可定制的Lua代码编辑检测工具

  * ## 第20章 安卓平台非托管内存分析方案

  * ## 第21章 过程化河流生成方法研究与应用
